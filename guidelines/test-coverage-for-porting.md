---
title: Test Coverage for Porting
description: Guidelines for maximizing test coverage before and during a Python-to-Rust port, including golden testing and tryscript
---
# Test Coverage for Porting

Guidelines for building comprehensive test coverage as the specification for a
Python-to-Rust port. High test coverage in the Python source directly translates to a
higher-quality, more automatable port.

For golden testing patterns, see `tbd guidelines golden-testing-guidelines` (guidelines/golden-testing-guidelines.md).
For general testing rules, see `tbd guidelines general-testing-rules` (guidelines/general-testing-rules.md).
For TDD methodology, see `tbd guidelines general-tdd-guidelines` (guidelines/general-tdd-guidelines.md).
For porting rules, see `tbd guidelines python-to-rust-porting-rules` (guidelines/python-to-rust-porting-rules.md).

## Core Principle

**Tests are the specification for the port.** Every test you write in Python before
porting becomes an exact requirement for the Rust implementation. Higher Python test
coverage = more precise Rust specification = fewer bugs in the port.

## Pre-Port Coverage Strategy

### Step 1: Measure Baseline Coverage

```bash
cd python-repo
uv run pytest --cov=myproject --cov-branch --cov-report=term-missing
```

Target: identify uncovered code paths that will need tests.

### Step 2: Write Unit Tests for Uncovered Code

Focus on:
- Edge cases in parsing/formatting logic
- Error handling paths
- Boundary conditions (empty input, huge input, special characters)
- Unicode handling

### Step 3: Write Integration Tests (CLI Golden Tests)

Capture the CLI's behavior as golden test fixtures:

```bash
# Generate golden output from known inputs
for f in test-fixtures/input/*.md; do
    python -m myproject "$f" > "test-fixtures/expected/$(basename $f)"
done
```

These expected outputs become the specification for the Rust port.

### Step 4: Use tryscript for CLI Golden Tests

[tryscript](https://github.com/jlevy/tryscript) captures CLI sessions as reproducible
test scripts:

```bash
# Install tryscript
pip install tryscript

# Record a CLI session
tryscript record test-session.try

# Replay and verify
tryscript test test-session.try
```

tryscript is particularly valuable for:
- Complex CLI workflows (multiple commands, piping)
- Interactive sessions
- Testing error messages and exit codes
- Regression testing after changes

### Step 5: Document Manual Test Procedures

For behaviors that are hard to automate (visual output, interactive sessions), create
documented manual test procedures:

```markdown
## Manual Test: Progress Bar Display

1. Run: `myproject --verbose large-file.md`
2. Verify: Progress bar appears on stderr
3. Verify: Progress bar shows file count
4. Verify: No progress bar when piped: `myproject large-file.md | cat`
```

## Test Fixture Organization

### Directory Structure

```
test-fixtures/
├── input/                    # Input files (shared between Python and Rust)
│   ├── basic.md
│   ├── complex-formatting.md
│   ├── edge-cases.md
│   ├── unicode.md
│   └── empty.md
├── expected/                 # Expected outputs (generated by Python)
│   ├── basic.md
│   ├── complex-formatting.md
│   └── ...
└── scripts/                  # Test scripts (tryscript files)
    ├── basic-workflow.try
    └── error-handling.try
```

### Fixture Management Best Practices

- **Check fixtures into version control.** Fixtures are the specification -- they must
  be versioned alongside the code so that changes are visible in diffs.
- **Regenerate, do not hand-edit.** Always regenerate expected outputs from the Python
  source of truth. Hand-editing fixtures drifts from actual behavior.
- **Keep fixtures small and focused.** Each fixture should test one concern. Avoid
  mega-fixtures that make failures hard to diagnose.
- **Handle binary fixtures carefully.** If your tool produces binary output (images,
  serialized data), store fixtures with Git LFS or a `.gitattributes` binary marker,
  and compare via checksums rather than byte-for-byte diffs.
- **Share fixtures between Python and Rust.** Place `test-fixtures/` at the repository
  root so both the Python test suite and Rust integration tests reference the same
  directory. This guarantees the same inputs and expected outputs are used by both
  implementations.

### Fixture Generation Script

```bash
#!/usr/bin/env bash
# scripts/generate-fixtures.sh
set -euo pipefail

PY_CMD=(uv run -q --project python-repo python -m myproject)

for input in test-fixtures/input/*.md; do
    name="$(basename "$input")"
    echo "Generating expected output for $name..."
    "${PY_CMD[@]}" "$input" > "test-fixtures/expected/$name"
done

echo "Generated $(ls test-fixtures/expected/ | wc -l) fixture(s)"
```

## Porting Tests to Rust

### Unit Tests

Port Python unit tests directly to Rust `#[test]` functions:

```python
# Python
def test_wrap_basic():
    assert wrap("hello world", width=5) == "hello\nworld"
```

```rust
// Rust
#[test]
fn test_wrap_basic() {
    assert_eq!(wrap("hello world", 5), "hello\nworld");
}
```

### Integration Tests with Fixtures

Use `include_str!()` to embed golden test fixtures:

```rust
#[test]
fn test_format_basic() {
    let input = include_str!("../../test-fixtures/input/basic.md");
    let expected = include_str!("../../test-fixtures/expected/basic.md");
    let result = format_document(input, &Config::default());
    assert_eq!(result, expected);
}
```

### Snapshot Testing with insta

The [`insta`](https://insta.rs/) crate is the standard for snapshot testing in Rust.
It works similarly to golden tests but manages snapshots automatically, making it
an excellent complement to the fixture-based approach above:

```rust
use insta::assert_snapshot;

#[test]
fn test_format_basic() {
    let input = include_str!("../../test-fixtures/input/basic.md");
    let result = format_document(input, &Config::default());
    assert_snapshot!(result);
}
```

Run `cargo insta review` to interactively accept or reject snapshot changes.
This is especially useful during active porting when outputs are stabilizing.

Add to `Cargo.toml`:
```toml
[dev-dependencies]
insta = "1"
proptest = "1"
# quickcheck = "1"  # Alternative to proptest
```

### Property-Based Tests

Use property-based testing to verify algorithmic invariants that are hard to cover
exhaustively with hand-written examples. The two main crates are:

- [`proptest`](https://crates.io/crates/proptest) -- Hypothesis-inspired, with
  per-value strategies and constraint-aware shrinking. Recommended for most porting
  projects because its strategy model makes it easy to express input constraints.
- [`quickcheck`](https://crates.io/crates/quickcheck) -- generates and shrinks
  values based on type alone. Simpler to set up but less flexible for constrained
  inputs.

Example using `proptest`:

```rust
use proptest::prelude::*;

proptest! {
    #[test]
    fn wrap_never_exceeds_width(s in "\\PC{1,200}", width in 10..200usize) {
        let wrapped = wrap(&s, width);
        for line in wrapped.lines() {
            // Lines should not exceed width (unless a single word is longer)
            prop_assert!(
                line.len() <= width || !line.contains(' '),
                "Line too long: {} (width: {})", line.len(), width
            );
        }
    }
}
```

## Coverage Tools for Rust

### cargo-llvm-cov (Recommended)

`cargo-llvm-cov` is the preferred coverage tool for Rust. It uses LLVM source-based
instrumentation, which provides more accurate results than alternatives and supports
line, region, and branch coverage. It works across Linux, macOS, and Windows:

```bash
cargo install cargo-llvm-cov
cargo llvm-cov --html --all-features          # Line coverage (HTML report)
cargo llvm-cov --html --all-features --branch  # Branch coverage (HTML report)
```

### cargo-tarpaulin (Linux Alternative)

`cargo-tarpaulin` is a Linux-focused alternative. Its default ptrace-based backend
works only on x86_64 Linux, though it also supports an LLVM backend via
`--engine llvm`. It is a reasonable choice if you are already using it in CI:

```bash
cargo install cargo-tarpaulin
cargo tarpaulin --out html --all-features
```

### Coverage Targets

| Component | Target | Rationale |
| --- | --- | --- |
| Core library | ≥90% lines | Critical business logic |
| Public API | 100% | Every public function tested |
| CLI wrapper | ≥80% | I/O paths are hard to fully cover |
| Error paths | ≥70% | Important but some are hard to trigger |

## Cross-Validation as Continuous Testing

Cross-validation (running both Python and Rust against the same inputs) should be part
of the CI pipeline during development:

```yaml
# .github/workflows/cross-validate.yml
cross-validate:
  runs-on: ubuntu-latest
  steps:
    - uses: actions/checkout@v6
      with:
        submodules: recursive
    - uses: dtolnay/rust-toolchain@stable
    - uses: astral-sh/setup-uv@v7
    - run: cargo build --release
    - run: cd python-repo && uv sync
    - run: ./scripts/cross-validate.sh
```

## Test Categories Checklist

Before porting, ensure the Python test suite covers:

- [ ] **Happy path** -- typical inputs produce expected outputs
- [ ] **Edge cases** -- empty input, single character, very long lines
- [ ] **Unicode** -- non-ASCII characters, emoji, CJK text
- [ ] **Special Markdown** -- code blocks, tables, footnotes, HTML, frontmatter
- [ ] **CLI flags** -- all combinations of flags and options
- [ ] **Error cases** -- missing files, permission errors, invalid input
- [ ] **Idempotency** -- running the tool twice produces the same output
- [ ] **Stdin/stdout** -- piping behavior matches file behavior
- [ ] **Large files** -- performance doesn't degrade on realistic inputs

## Increasing Coverage During the Port

As you port each module:

1. **Port existing tests** -- these are the specification
2. **Add Rust-specific tests** -- for Rust edge cases (ownership, lifetimes)
3. **Add property tests** -- use proptest (or quickcheck) for algorithmic invariants
4. **Run cross-validation** -- catch differences unit tests miss
5. **Measure coverage** -- fill gaps discovered by cargo-llvm-cov (preferred) or tarpaulin

## Related Guidelines

- For golden testing patterns, see `tbd guidelines golden-testing-guidelines` (guidelines/golden-testing-guidelines.md)
- For general testing rules, see `tbd guidelines general-testing-rules` (guidelines/general-testing-rules.md)
- For TDD methodology, see `tbd guidelines general-tdd-guidelines` (guidelines/general-tdd-guidelines.md)
- For porting rules, see `tbd guidelines python-to-rust-porting-rules` (guidelines/python-to-rust-porting-rules.md)
- For CLI porting, see `tbd guidelines python-to-rust-cli-porting` (guidelines/python-to-rust-cli-porting.md)
